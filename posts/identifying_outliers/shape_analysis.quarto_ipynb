{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Farm Shape Analysis: Linking Geometry with Crop Yield and Diversity\"\n",
        "jupyter: python3\n",
        "\n",
        "author:\n",
        "  - name: \"Mo Wang\"\n",
        "\n",
        "date: \"December 15, 2024\"\n",
        "categories: [landscape-analysis, agriculture]\n",
        "\n",
        "callout-icon: false\n",
        "format:\n",
        "  html:\n",
        "    mathjax: true\n",
        "    code-fold: true\n",
        "    allow-html: true\n",
        "bibliography: references.bib\n",
        "\n",
        "\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "  freeze: auto\n",
        "  warning: false\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 1. Introduction and Motivation\n",
        "\n",
        "In modern agriculture, the geometric features of farmland play a crucial role in farm management and planning. Understanding these characteristics enables farmers to make informed decisions, manage resources more efficiently, and promote sustainable agricultural practices.\n",
        "\n",
        "This research leverages data from [Litefarm](https://www.litefarm.org/), an open-source agri-tech application designed to support sustainable agriculture. Litefarm provides detailed information about farmland, including field shapes, offering valuable insights for analysis. However, as an open platform, Litefarm's database may include unrealistic or inaccurate data entries, such as \"fake farms.\" Cleaning and validating this data is essential for ensuring the reliability of agricultural analyses.\n",
        "\n",
        "In this blog, we focus on identifying fake farms by analyzing field shapes to detect unrealistic entries. Our goal is to enhance data accuracy, providing a stronger foundation for future agriculture-related research.\n",
        "\n",
        "![Litefarm Interface](img/Litefarm.png)\n",
        "\n",
        "## 2. Dataset Overview and Preparation\n",
        "\n",
        "### Data Source\n",
        "\n",
        "The data for this study was extracted from Litefarm's database, which contains detailed information about farm geometries, locations, and user associations. The dataset included the following key attributes:\n",
        "\n",
        "-   **Farm-Level Information**:\\\n",
        "    Each farm is uniquely identified by a **farm_ID**, representing an individual farm within the Litefarm database.\n",
        "\n",
        "-   **Polygon-Level Information**:\\\n",
        "    Each farm consists of multiple polygons, corresponding to distinct areas such as **fields**, **gardens**, or **barns**. Each polygon is uniquely identified by a **location_ID**, ensuring that every area within a farm is individually traceable.\n",
        "\n",
        "-   **Geometric Attributes**:\n",
        "\n",
        "    -   **Area**: The total surface area of the polygon.\\\n",
        "    -   **Perimeter**: The boundary length of the polygon.\n",
        "\n",
        "-   **Vertex Coordinates**:\\\n",
        "    The geographic shape of each polygon is defined by a list of vertex coordinates in latitude and longitude format, represented as: `[(lat1, lon1), (lat2, lon2), ..., (latN, lonN)]`.\n",
        "\n",
        "-   **Polygon Types**:\\\n",
        "    The polygons in each farm are categorized into various types:\n",
        "\n",
        "    -   **Fields**\\\n",
        "    -   **Farm site boundaries**\\\n",
        "    -   **Residences**\\\n",
        "    -   **Barns**\\\n",
        "    -   **Gardens**\\\n",
        "    -   **Surface water**\\\n",
        "    -   **Natural areas**\\\n",
        "    -   **Greenhouses**\\\n",
        "    -   **Ceremonial areas**\n",
        "\n",
        "This rich dataset captures farm structures and geometries comprehensively, enabling the analysis of relationships between polygon features and agricultural outcomes.\n",
        "\n",
        "This study focuses specifically on **productive areas**—gardens, greenhouses, and fields—as these contribute directly to agricultural output. Since different polygon types possess unique geometric characteristics, we focused on a single type to maintain analytical consistency.\n",
        "\n",
        "As the Litefarm database is dynamic and continuously updated, the data captured as of November 28th showed that 36.4% of farms included garden areas, 20.7% had greenhouse areas, and nearly 70% contained fields. To ensure a robust and representative analysis, we focused on field polygons, which had the highest proportion within the dataset.\n",
        "\n",
        "### Refined Litefarm Dataset\n",
        "\n",
        "To ensure that only valid and realistic farm data was included in the analysis, we applied rigorous SQL filters to the Litefarm database. These filters excluded:\n",
        "\n",
        "-   Placeholder farms and internal test accounts.\\\n",
        "-   Deleted records.\\\n",
        "-   Farms located in countries with insufficient representation (fewer than 10 farms).\n",
        "\n",
        "The table below summarizes the results of the filtering process and the composition of the cleaned dataset:\n",
        "\n",
        "| Description                         | Count |\n",
        "|-------------------------------------|-------|\n",
        "| Initial number of farms in Litefarm | 3,559 |\n",
        "| Farms after SQL filtering           | 2,919 |\n",
        "| Farms with field areas              | 2,022 |\n",
        "| Farms with garden areas             | 1,063 |\n",
        "| Farms with greenhouse areas         | 607   |\n",
        "| Total number of field polygons      | 6,340 |\n",
        "\n",
        "By narrowing the focus to field polygons, we ensured that the dataset was both robust and suitable for exploring the relationship between geometric features and agricultural outcomes.\n",
        "\n",
        "## 3. Shape Analysis\n",
        "\n",
        "This study focuses on the geometric properties of field polygons, as these are essential for understanding farm structures and ensuring data reliability. Each field polygon is represented by a series of vertices in latitude-longitude pairs, which outline its geometric boundaries. These vertices are the foundation for calculating key metrics such as **area**, **perimeter**, and more complex shape properties.\n",
        "\n",
        "To perform a robust analysis, we systematically processed and evaluated the field polygon data through the following steps:\n",
        "\n",
        "### 1. Vertex Distribution Analysis\n",
        "\n",
        "The first step in our analysis was to examine the **vertex distribution** of the field polygons to understand their general characteristics and ensure data quality. A box plot was created to visualize the distribution of vertex counts: ![boxplot of vertices' number distribution](img/boxplot.png)\n",
        "\n",
        "The results revealed a wide range of vertex counts, spanning from 3 to 189 vertices. This variability required filtering to address potential outliers. Using the **z-score method**, we identified and excluded extreme values, capping the maximum vertex count at 34.\n",
        "\n",
        "After filtering, we analyzed the revised vertex distribution using a histogram, which revealed that 47.4% of field polygons had exactly four vertices:\n",
        "\n",
        "![histogram of number of veritces](img/number_of_vertices.png)\n",
        "\n",
        "### 2. Validation of Area and Perimeter Metrics\n",
        "\n",
        "#### Recalculation Process:\n",
        "\n",
        "1.  Vertex coordinates, initially in latitude-longitude format, were transformed into a planar coordinate system (`EPSG:6933`) to enable precise calculations.\n",
        "2.  **Area** and **perimeter** were computed directly from the transformed vertex data.\n",
        "\n",
        "Scatter plots comparing the user-provided values with the recalculated metrics showed strong alignment, with most points clustering around the diagonal (dashed line), confirming the accuracy of the recalculated values:\n",
        "\n",
        "-   **Perimeter Comparison**\\\n",
        "    ![Perimeter Scatter Plot](img/perimeter1.png)\n",
        "\n",
        "-   **Area Comparison**\\\n",
        "    ![Area Scatter Plot](img/area1.png)\n",
        "\n",
        "This validation step provided confidence in the accuracy of the recalculated metrics, allowing us to proceed with subsequent shape analysis using reliable data.\n",
        "\n",
        "## Field Polygon Standardization and Preparation\n",
        "\n",
        "To focus on the geometric properties of field polygons, we projected all polygons into a **size-and-shape space**. This transformation isolates the shape and scale of the polygons while removing variations caused by rotation and translation. The size-and-shape space ensures consistent and meaningful comparisons of the underlying geometric features.\n",
        "\n",
        "While this study emphasizes polygon shapes, we recognize that **area** is a critical feature in agricultural studies due to its relationship with factors like regional regulations and agricultural policies. Thus, we preserved the size (scaling) component in our analysis to maintain the relevance of area.\n",
        "\n",
        "To ensure uniformity and consistency in the dataset, we performed the following preprocessing steps:\n",
        "\n",
        "1.  **Standardizing Landmark Points**:\n",
        "\n",
        "To enable meaningful comparisons in the size-and-shape space, each polygon was resampled to have exactly 34 evenly spaced points along its boundary. The following Python function illustrates this process:\n"
      ],
      "id": "c323c1f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "import folium\n",
        "import json\n",
        "from shapely.geometry import shape, Polygon, Point, MultiPoint, MultiPolygon, LineString,LinearRing, MultiLineString\n",
        "from shapely.ops import unary_union, transform, nearest_points\n",
        "from collections import defaultdict\n",
        "import geopy.distance\n",
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "import itertools\n",
        "import pyproj\n",
        "from functools import partial\n",
        "from collections import defaultdict\n",
        "import altair as alt\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objs as go\n",
        "from pyproj import Transformer, CRS \n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import logging\n",
        "from shapely.validation import explain_validity\n",
        "import geopandas as gpd\n",
        "import ast\n",
        "from geographiclib.geodesic import Geodesic\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from geopy.distance import geodesic\n",
        "from geomstats.geometry.pre_shape import PreShapeSpace\n",
        "from geomstats.visualization import KendallDisk, KendallSphere"
      ],
      "id": "89f9a752",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "def resample_polygon(projected_coords, num_points=34):\n",
        "    \"\"\"\n",
        "    Resample a polygon's boundary to have a specified number of evenly spaced points.\n",
        "\n",
        "    Parameters:\n",
        "    - projected_coords: List of coordinates defining the polygon's boundary.\n",
        "    - num_points: The number of evenly spaced points to resample (default is 34).\n",
        "\n",
        "    Returns:\n",
        "    - new_coords: List of resampled coordinates.\n",
        "    \"\"\"\n",
        "    ring = LinearRing(projected_coords)\n",
        "    \n",
        "    total_length = ring.length\n",
        "\n",
        "    distances = np.linspace(0, total_length, num_points, endpoint=False)\n",
        "    \n",
        "    new_coords = [ring.interpolate(distance).coords[0] for distance in distances]\n",
        "    \n",
        "    return new_coords"
      ],
      "id": "2790174e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.  **Ensuring Consistent Vertex Direction**:\n",
        "\n",
        "All polygons were standardized to have vertices drawn in the same direction (clockwise or counterclockwise). This step ensures that the orientation of the vertices does not introduce inconsistencies in the analysis.\n"
      ],
      "id": "147303ad"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "def is_clockwise(coords):\n",
        "    \"\"\"\n",
        "    Check if the polygon vertices are in a clockwise direction.\n",
        "\n",
        "    Parameters:\n",
        "    - coords: List of coordinates defining the polygon's boundary.\n",
        "\n",
        "    Returns:\n",
        "    - True if the polygon is clockwise; False otherwise.\n",
        "    \"\"\"\n",
        "    ring = LinearRing(coords)\n",
        "    return ring.is_ccw == False \n",
        "\n",
        "def make_clockwise(coords):\n",
        "    \"\"\"\n",
        "    Convert the polygon's vertices to a clockwise direction, if it is not \n",
        "\n",
        "    Parameters:\n",
        "    - coords: List of coordinates defining the polygon's boundary.\n",
        "\n",
        "    Returns:\n",
        "    - List of coordinates ordered in a clockwise direction.\n",
        "    \"\"\"\n",
        "    if not is_clockwise(coords):  \n",
        "        return [coords[0]] + coords[:0:-1]  # Reverse the vertex order, keeping the start point\n",
        "    return coords"
      ],
      "id": "db8ccff7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The image illustrates four polygons that have been standardized by resampling them to have 34 evenly spaced points, with all vertices aligned in a clockwise direction.\n",
        "\n",
        "![The standardized polygon](img/resampled_polygon.png)\n",
        "\n",
        "#### Validation of Standardization\n",
        "\n",
        "To confirm the accuracy of these transformations, we compared the areas and perimeters of the resampled polygons with the original values. The results demonstrated minimal deviation, indicating the transformations preserved the integrity of the shapes.\n",
        "\n",
        "-   **Perimeter Comparison**\n",
        "\n",
        "![perimeter comparison](img/resamples_perimeter.png)\n",
        "\n",
        "-   **Area Comparison**\n",
        "\n",
        "![area comparison](img/resampled_area.png)\n",
        "\n",
        "By meeting these preprocessing requirements, we ensured that the polygons were accurately prepared for subsequent shape analysis.\n",
        "\n",
        "## Shape Alignment and Fréchet Mean Analysis\n",
        "\n",
        "With data preparation complete, the polygons were ready for analysis in the **size-and-shape space**. This specialized framework enables consistent comparison of shapes by accounting for geometric differences, including scaling, translation, and rotation. It provides a robust foundation for meaningful geometric analysis.\n",
        "\n",
        "The polygons were aligned using **Procrustes analysis**[@dryden2016shape], and their **Fréchet Mean** was iteratively computed in Euclidean space. This process standardizes the shapes, ensuring variations caused by translation and rotation are removed, allowing for accurate and meaningful comparisons.\n",
        "\n",
        "The **Fréchet Mean**[@dryden2016shape] represents the \"average\" shape in a geometric space (manifold), minimizing the average squared distance to all sample shapes. It serves as a standardized and central representation of the dataset.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "#### Step-by-Step Overview\n",
        "\n",
        "1.  **Shape Alignment**:\n",
        "    -   The `align_shape` function performs Procrustes alignment through the following steps:\n",
        "        1.  **Removing Translation**:\n",
        "            -   The centroid (average position of all points) of each shape is computed. The shape is then centered by subtracting its centroid from all points, ensuring the shape is position-independent.\n",
        "        2.  **Removing Rotation**:\n",
        "            -   Using Singular Value Decomposition (SVD), the optimal rotation matrix is calculated to align the target shape with the reference shape. This step removes rotation differences while preserving the relative positions of the points.\n",
        "2.  **Measuring Shape Differences**:\n",
        "    -   The `riemannian_distance` function computes the **Riemannian distance** between two shapes in size-and-shape space. This metric quantifies geometric differences between shapes, considering both size and rotation.\n",
        "\n",
        "##### Riemannian Distance in Size-and-Shape Space\n",
        "\n",
        "Given two $k$-point configurations in $m$-dimensions, $X_1^o, X_2^o \\in \\mathbb{R}^{k \\times m}$, the **Riemannian distance**[@dryden2016shape] in size-and-shape space is defined as:\n",
        "\n",
        "$$\n",
        "d_S(X_1^o, X_2^o) = \\sqrt{S_1^2 + S_2^2 - 2 S_1 S_2 \\cos \\rho(X_1^o, X_2^o)}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "-   $S_1, S_2$: Centroid sizes of $X_1^o$ and $X_2^o$, representing the Frobenius norms of the centered shapes.\n",
        "\n",
        "-   $\\rho(X_1^o, X_2^o)$: Riemannian shape distance.\n",
        "\n",
        "This formula ensures that the distance captures both shape similarity and scaling differences, making it a robust tool for geometric analysis.\n",
        "\n",
        "3.  **Iterative Fréchet Mean Calculation**:\n",
        "    -   The algorithm begins with an initial **reference shape** and aligns all other shapes to it using Procrustes alignment.\n",
        "    -   The Fréchet Mean is then calculated as the average shape in Euclidean space.\n",
        "    -   The shapes are iteratively re-aligned to the updated Fréchet Mean, refining the alignment and mean calculation until convergence is achieved.\n",
        "\n",
        "#### Python Implementation\n",
        "\n",
        "The following Python code implements the entire process of shape alignment, Riemannian distance computation, and iterative Fréchet Mean calculation.\n"
      ],
      "id": "afb50371"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "def align_shape(reference_shape, target_shape):\n",
        "    \"\"\"\n",
        "    Align the target shape to the reference shape using Procrustes alignment.\n",
        "\n",
        "    Parameters:\n",
        "    - reference_shape: The reference shape to align to.\n",
        "    - target_shape: The shape to be aligned.\n",
        "\n",
        "    Returns:\n",
        "    - aligned_shape: The aligned target shape.\n",
        "    \"\"\"\n",
        "    reference_shape = np.array(reference_shape)\n",
        "    target_shape = np.array(target_shape)\n",
        "\n",
        "    # Step 1: Remove the translation\n",
        "    centroid_reference = np.mean(reference_shape, axis=0)\n",
        "    centroid_target = np.mean(target_shape, axis=0)\n",
        "    centered_reference = reference_shape - centroid_reference\n",
        "    centered_target = target_shape - centroid_target\n",
        "\n",
        "    # Step 2: Remove the rotation\n",
        "    u, s, vh = np.linalg.svd(np.matmul(np.transpose(centered_target), centered_reference))\n",
        "    r = np.matmul(u, vh)\n",
        "    aligned_shape = np.matmul(centered_target, r)\n",
        "\n",
        "    return aligned_shape\n",
        "\n",
        "def riemannian_distance(reference_shape, target_shape):\n",
        "    \"\"\"\n",
        "    Compute the Riemannian distance between two shapes.\n",
        "\n",
        "    Parameters:\n",
        "    - reference_shape: The reference shape.\n",
        "    - target_shape: The target shape.\n",
        "\n",
        "    Returns:\n",
        "    - distance: The Riemannian distance between the shapes.\n",
        "    \"\"\"\n",
        "    reference_shape = np.array(reference_shape)\n",
        "    target_shape = np.array(target_shape)\n",
        "\n",
        "    # Step 1: Compute centroid sizes\n",
        "    S1 = np.linalg.norm(reference_shape)  \n",
        "    S2 = np.linalg.norm(target_shape)\n",
        "\n",
        "    # Step 2: Remove translation by centering the shapes\n",
        "    centered_reference = reference_shape - np.mean(reference_shape, axis=0)\n",
        "    centered_target = target_shape - np.mean(target_shape, axis=0)\n",
        "\n",
        "    # Step 3: Compute optimal rotation using SVD\n",
        "    H = np.dot(centered_target.T, centered_reference)\n",
        "    U, _, Vt = np.linalg.svd(H)\n",
        "    R = np.dot(U, Vt)\n",
        "\n",
        "    # Step 4: Align target shape\n",
        "    aligned_target = np.dot(centered_target, R)\n",
        "\n",
        "    # Step 5: Compute the Riemannian distance\n",
        "    cosine_rho = np.trace(np.dot(aligned_target.T, centered_reference)) / (S1 * S2)\n",
        "    cosine_rho = np.clip(cosine_rho, -1, 1)\n",
        "    distance = np.sqrt(S1**2 + S2**2 - 2 * S1 * S2 * cosine_rho)\n",
        "\n",
        "    return distance\n",
        "\n",
        "# Iterative Fréchet Mean Calculation\n",
        "epsilon = 1e-6  \n",
        "max_iterations = 100  \n",
        "reference_shape = field_data['resampled_point'].iloc[0]  \n",
        "aligned_shapes = []\n",
        "\n",
        "# Align all shapes to the initial reference shape\n",
        "for target_shape in field_data['resampled_point']:\n",
        "    aligned_shape = align_shape(reference_shape, target_shape)\n",
        "    aligned_shapes.append(aligned_shape)\n",
        "\n",
        "# Initialize Euclidean space and calculate initial Fréchet Mean\n",
        "euclidean_space = Euclidean(dim=aligned_shapes[0].shape[1])\n",
        "frechet_mean = FrechetMean(euclidean_space)\n",
        "previous_frechet_mean_shape = frechet_mean.fit(aligned_shapes).estimate_\n",
        "converged = False\n",
        "iteration = 0\n",
        "frechet_means = [previous_frechet_mean_shape]\n",
        "\n",
        "while not converged and iteration < max_iterations:\n",
        "    iteration += 1\n",
        "    aligned_shapes2 = []\n",
        "    for target_shape in field_data['resampled_point']:\n",
        "        aligned_shape = align_shape(previous_frechet_mean_shape, target_shape)\n",
        "        aligned_shapes2.append(aligned_shape)\n",
        "\n",
        "    # Calculate new Fréchet Mean\n",
        "    frechet_mean = FrechetMean(euclidean_space)\n",
        "    current_frechet_mean_shape = frechet_mean.fit(aligned_shapes2).estimate_\n",
        "    frechet_means.append(current_frechet_mean_shape)\n",
        "    \n",
        "    # Check convergence\n",
        "    difference = riemannian_distance(previous_frechet_mean_shape, current_frechet_mean_shape)\n",
        "    if difference < epsilon:\n",
        "        converged = True\n",
        "    else:\n",
        "        previous_frechet_mean_shape = current_frechet_mean_shape"
      ],
      "id": "a01d4bd2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Global Fréchet Mean and Outlier Detection\n",
        "\n",
        "Here is the global Fréchet mean calculated from all field polygons:\n",
        "\n",
        "![The global mean shape](img/mean_shape.png)\n",
        "\n",
        "The following image illustrates the original polygon and its alignment with the Fréchet mean:\n",
        "\n",
        "![Aligned Shape](img/aligned_shape.png)\n",
        "\n",
        "After aligning all shapes to the Fréchet mean, the `riemannian_distance` function was used to calculate the distances between the mean shape and each aligned shape. To identify potential outliers, the z-score method was applied to these distance values.\n",
        "\n",
        "Below are the four field polygons detected as outliers using the global Fréchet mean:\n"
      ],
      "id": "1d2f3812"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "four_potiential_fake_farm = pd.read_csv(\"/Users/prabhjotsingh/LiteFarm-Data-Processing-Guide/posts/identifying_outliers/data/potiential_fake_field.csv\")\n",
        "\n",
        "# Display the table\n",
        "four_potiential_fake_farm   # Or use `data` to show the entire table"
      ],
      "id": "7453fc28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fréchet Mean Shape by Country\n",
        "\n",
        "The shape of field polygons varies significantly across different countries. To capture this variation, we calculated the **Fréchet mean shape**\\* for each country based on the fields located within that specific country.\n",
        "\n",
        "The plot below summarizes the Fréchet mean shapes for all countries in the dataset.\n",
        "\n",
        "In this visualization, **different colors represent different continents**. It is evident that both the shapes and areas of the field polygons differ substantially across regions, highlighting the diversity in field geometry across countries.\n",
        "\n",
        "![Summary of Countries' Mean Shapes](img/output.png)\n",
        "\n",
        "### Assessing Mean Shape Representation in Countries with Limited Data\n",
        "\n",
        "To evaluate the representativeness of the mean shape, we specifically selected countries with fewer than 10 polygons. The small number of polygons in these cases allows for easier visualization, helping us assess whether the mean shape effectively captures the overall geometric characteristics of these datasets.\n",
        "\n",
        "#### Zambia\n",
        "\n",
        "![Field polygons and Fréchet mean for Zambia](img/small_number_country1.png)\n",
        "\n",
        "#### Chile\n",
        "\n",
        "![Field polygons and Fréchet mean for Chile](img/small_number_country.png)\n",
        "\n",
        "From the above plots, we can draw the following conclusions:\n",
        "\n",
        "-   **Effective Representation with Similar Shapes**:\n",
        "\n",
        "    When the field polygons within a country have similar shapes, the calculated Fréchet mean serves as an effective representation of the general shape trend.\n",
        "\n",
        "-   **Limitations with Diverse Shapes**:\n",
        "\n",
        "    If the field polygons within a country show significant variation in their shapes, the Fréchet mean becomes less representative and may fail to adequately capture the geometric diversity of the dataset.\n",
        "\n",
        "### Detecting Potential Fake Field Polygons\n",
        "\n",
        "Building on the country-level mean shape analysis, we applied the same methodology to detect potential fake field polygons. For each country, field polygons were aligned to their corresponding **Fréchet mean**, and the **z-score technique** was used to identify anomalies based on the distances between each polygon and the mean shape.\n",
        "\n",
        "Through this analysis, we identified **51 potential fake field polygons**. To verify their validity, we visualized each field polygon on satellite imagery. The results are summarized in the plot below:\n",
        "\n",
        "-   **Gray markers**: Fake fields\\\n",
        "-   **Pink markers**: True fields\\\n",
        "-   **Orange markers**: Potential fake fields\n",
        "\n",
        "![Satellite plot for all 51 potential fake fields](img/satelite.png)\n",
        "\n",
        "After visualizing all 51 potential fake field polygons, the findings were as follows:\n",
        "\n",
        "-   **45.1%** were confirmed as fake fields.\\\n",
        "-   **29.4%** were ambiguous, meaning they could potentially be either fake or real fields, requiring further investigation.\n",
        "-   **25.5%** were determined to be true fields.\n",
        "\n",
        "Below are examples of confirmed fake fields. These polygons often exhibit:\n",
        "\n",
        "-   **Unusual geometric shapes**\n",
        "\n",
        "-   **Sizes that are disproportionately large compared to neighboring field polygons**\n",
        "\n",
        "![fake field polygons](img/fake_farm.jpg)\n",
        "\n",
        "### Future Work\n",
        "\n",
        "Our analysis successfully identified a significant number of potential fake field polygons, with nearly half of these cases being validated as genuinely fake. While this demonstrates the effectiveness of our approach, there is still room to improve the accuracy and reliability of the detection process. To further refine our results, future efforts will focus on:\n",
        "\n",
        "1.  **Incorporate Geographic Information**:\\\n",
        "    Enrich the dataset with geographic features such as proximity to natural landmarks (e.g., mountains, rivers) or man-made structures (e.g., urban areas, roads). These features could provide valuable context for improving the calculation of the Fréchet mean and detecting anomalies more effectively.\n",
        "2.  **Improve Outlier Detection Methods**:\\\n",
        "    Leverage advanced machine learning models, such as clustering algorithms or ensemble methods, to identify subtle patterns and relationships that may indicate fake fields. Techniques like unsupervised learning or deep anomaly detection could also be explored to improve performance.\n",
        "\n",
        "### References"
      ],
      "id": "1762eb19"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/prabhjotsingh/Library/Python/3.9/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}